{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 20:38:01,215 - INFO - Starting data processing...\n",
      "2023-11-14 20:38:01,216 - ERROR - Error processing supplier data: [Errno 2] No such file or directory: '/supplier.csv'\n",
      "2023-11-14 20:38:01,217 - ERROR - Supplier data processing failed. Exiting process.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "# Configurations\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "load_dotenv()\n",
    "\n",
    "# Environment variables\n",
    "client_id = os.getenv(\"BIGCOMMERCE_CLIENT_ID\")\n",
    "access_token = os.getenv(\"BIGCOMMERCE_ACCESS_TOKEN\")\n",
    "store_hash = os.getenv(\"BIGCOMMERCE_STORE_HASH\")\n",
    "BASE_URL_TEMPLATE = f\"https://api.bigcommerce.com/stores/{store_hash}/v3/\"\n",
    "\n",
    "class BigCommerceAPI:\n",
    "    def __init__(self, client_id, access_token, store_hash):\n",
    "        self.headers = {\n",
    "            \"X-Auth-Client\": client_id,\n",
    "            \"X-Auth-Token\": access_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.base_url = BASE_URL_TEMPLATE.format(store_hash=store_hash)\n",
    "\n",
    "    def get_request(self, url):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except HTTPError as http_err:\n",
    "            logging.error(f\"HTTP error occurred: {http_err}, Response: {response.text}\")\n",
    "        except Exception as err:\n",
    "            logging.error(f\"An error occurred: {err}\")\n",
    "            return None\n",
    "\n",
    "    # [Add post_request, put_request, delete_request methods if needed]\n",
    "\n",
    "def parse_supplier_data(csv_file, json_file):\n",
    "    try:\n",
    "        supplier_data = pd.read_csv(csv_file)\n",
    "        with open(json_file, 'r') as file:\n",
    "            brands_dict = json.load(file)\n",
    "\n",
    "        filtered_data = []\n",
    "        for _, row in supplier_data.iterrows():\n",
    "            brand = row['Brand']\n",
    "            if brand in brands_dict.values():\n",
    "                filtered_data.append({\n",
    "                    \"SKU\": row['SKU'],\n",
    "                    \"Name\": row['Name'],\n",
    "                    \"Brand\": brand,\n",
    "                    \"Cost\": row['Cost']\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(filtered_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing supplier data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_bigcommerce_products(api):\n",
    "    # Assuming products' SKUs are stored in 'sku' field\n",
    "    products = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = api.get_request(f\"catalog/products?page={page}&limit=250\")\n",
    "        if not response or 'data' not in response:\n",
    "            break\n",
    "        products.extend(response['data'])\n",
    "        if not response['meta']['pagination']['next_page']:\n",
    "            break\n",
    "        page += 1\n",
    "    return {product['sku']: product for product in products if 'sku' in product}\n",
    "\n",
    "def compare_and_generate_csv(supplier_data, bigcommerce_data):\n",
    "    added_products = supplier_data[~supplier_data['SKU'].isin(bigcommerce_data.keys())]\n",
    "    removed_products = pd.DataFrame([sku for sku in bigcommerce_data.keys() if sku not in supplier_data['SKU'].values], columns=['SKU'])\n",
    "\n",
    "    changes_csv = 'changes.csv'\n",
    "    with pd.ExcelWriter(changes_csv) as writer:\n",
    "        added_products.to_excel(writer, sheet_name='Added Products', index=False)\n",
    "        removed_products.to_excel(writer, sheet_name='Removed Products', index=False)\n",
    "\n",
    "    return changes_csv\n",
    "\n",
    "def update_bigcommerce_store(api, changes_csv):\n",
    "    # This function should handle updating the store based on the changes\n",
    "    # For now, it's a placeholder\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting data processing...\")\n",
    "    api = BigCommerceAPI(client_id, access_token, store_hash)\n",
    "\n",
    "    supplier_file_path = \"supplier.csv\"\n",
    "    brands_json_path = \"brands.json\"\n",
    "    supplier_data = parse_supplier_data(supplier_file_path, brands_json_path)\n",
    "\n",
    "    if supplier_data.empty:\n",
    "        logging.error(\"Supplier data processing failed. Exiting process.\")\n",
    "        return\n",
    "\n",
    "    bigcommerce_data = fetch_bigcommerce_products(api)\n",
    "    changes_csv = compare_and_generate_csv(supplier_data, bigcommerce_data)\n",
    "    update_bigcommerce_store(api, changes_csv)\n",
    "\n",
    "    logging.info(f\"Data processing completed. Changes recorded in {changes_csv}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/5nmbkvdx2bj8lf_myl2bx1fc0000gq/T/ipykernel_70046/263930454.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deleted_products['Type'] = 'Deleted Product'\n",
      "/var/folders/yk/5nmbkvdx2bj8lf_myl2bx1fc0000gq/T/ipykernel_70046/263930454.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_products['Type'] = 'Product Updated'\n",
      "2023-11-16 20:09:15,066 - INFO - Output saved to: updated_product_list_final.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def match_product_name(supplier_name, threshold=0.8):\n",
    "    try:\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for store_name in products_df['StoreName']:\n",
    "            if pd.isna(supplier_name) or pd.isna(store_name):\n",
    "                continue\n",
    "            ratio = difflib.SequenceMatcher(None, supplier_name, store_name).ratio()\n",
    "            if ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = store_name\n",
    "        return best_match if best_ratio >= threshold else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in match_product_name: {e}\")\n",
    "        return None\n",
    "\n",
    "def contains_excluded_word(name, excluded_words):\n",
    "    try:\n",
    "        return any(word.lower() in name.lower() for word in excluded_words)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in contains_excluded_word: {e}\")\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    # Load the CSV and JSON files\n",
    "    products_path = 'IMPORT.csv'\n",
    "    suppliers_path = 'supplier.csv'\n",
    "    brands_path = 'brands.json'\n",
    "\n",
    "    # Reading the files\n",
    "    products_df = pd.read_csv(products_path)\n",
    "    suppliers_df = pd.read_csv(suppliers_path)\n",
    "\n",
    "    if 'Item' in products_df.columns:\n",
    "        products_df = products_df[products_df['Item'] != 'Image']\n",
    "    else:\n",
    "        logging.warning(\"'Item' column not found in products CSV.\")\n",
    "\n",
    "    products_df = products_df[['SKU', 'Name', 'Cost Price']]\n",
    "    products_df.columns = ['SKU', 'StoreName', 'StoreCost']\n",
    "    products_df = products_df.dropna(subset=['StoreName'])\n",
    "\n",
    "    suppliers_df = suppliers_df[['Nombre', 'Precio']]\n",
    "    suppliers_df.columns = ['Name', 'SupplierPrice']\n",
    "    suppliers_df = suppliers_df.dropna(subset=['Name'])\n",
    "\n",
    "    # Load and process brands data\n",
    "    with open(brands_path, 'r') as file:\n",
    "        brands_data = json.load(file)\n",
    "    brand_map = {brand['abbreviation']: brand['full_name'] for brand in brands_data}\n",
    "    suppliers_df['Brand'] = suppliers_df['Name'].apply(lambda x: x.split()[0] if pd.notna(x) else None)\n",
    "    suppliers_df = suppliers_df[suppliers_df['Brand'].isin(brand_map.keys())]\n",
    "\n",
    "    # Exclude products containing specific keywords\n",
    "    excluded_words = [\"Rebajado\", \"Oferta\", \"Promocion\", \"Antes\", \"Golpeado\"]\n",
    "    products_df = products_df[~products_df['StoreName'].apply(lambda x: contains_excluded_word(x, excluded_words))]\n",
    "    suppliers_df = suppliers_df[~suppliers_df['Name'].apply(lambda x: contains_excluded_word(x, excluded_words))]\n",
    "\n",
    "    # Apply fuzzy matching to find corresponding SKU in products_df for each supplier product\n",
    "    suppliers_df['MatchedName'] = suppliers_df['Name'].apply(match_product_name)\n",
    "    merged_df = pd.merge(suppliers_df, products_df, left_on='MatchedName', right_on='StoreName', how='left')\n",
    "\n",
    "    # Identifying new products, deleted products, and products with price updates\n",
    "    new_products = merged_df[merged_df['SKU'].isna()][['Name', 'SupplierPrice']]\n",
    "    new_products['Type'] = 'New Product'\n",
    "\n",
    "    deleted_products = products_df[~products_df['StoreName'].isin(merged_df['StoreName'])]\n",
    "    deleted_products['Type'] = 'Deleted Product'\n",
    "\n",
    "    updated_products = merged_df[~merged_df['SKU'].isna() & (merged_df['StoreCost'] != merged_df['SupplierPrice'])]\n",
    "    updated_products['Type'] = 'Product Updated'\n",
    "\n",
    "    # Preparing the final DataFrame\n",
    "    final_df = pd.concat([\n",
    "        new_products,\n",
    "        deleted_products[['SKU', 'StoreName', 'StoreCost', 'Type']].rename(columns={'StoreName': 'Name'}),\n",
    "        updated_products[['SKU', 'Name', 'StoreCost', 'SupplierPrice', 'Type']]\n",
    "    ])\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the final DataFrame to an Excel file\n",
    "    final_output_path = 'updated_product_list_final.xlsx'\n",
    "    final_df.to_excel(final_output_path, index=False)\n",
    "\n",
    "    logging.info(f\"Output saved to: {final_output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in main script: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:11:32,930 - INFO - Fetched Page 1: 250 products\n",
      "2023-11-16 21:11:35,274 - INFO - Fetched Page 1: 250 products\n",
      "2023-11-16 21:11:35,582 - ERROR - Error processing supplier data: 'Brand'\n",
      "2023-11-16 21:11:35,583 - ERROR - Supplier data is empty or could not be processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "# Configurations\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "BASE_URL_TEMPLATE = \"https://api.bigcommerce.com/stores/{store_hash}/v3/\"\n",
    "\n",
    "# BigCommerceAPI class for API interactions\n",
    "class BigCommerceAPI:\n",
    "    def __init__(self, client_id, access_token, store_hash):\n",
    "        self.headers = {\n",
    "            \"X-Auth-Client\": client_id,\n",
    "            \"X-Auth-Token\": access_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.base_url = BASE_URL_TEMPLATE.format(store_hash=store_hash)\n",
    "\n",
    "    def get_request(self, url):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except HTTPError as http_err:\n",
    "            logging.error(f\"HTTP error occurred: {http_err}, Response: {response.text}\")\n",
    "            return None\n",
    "        except Exception as err:\n",
    "            logging.error(f\"An error occurred: {err}\")\n",
    "            return None\n",
    "\n",
    "# Function to parse supplier data\n",
    "def parse_supplier_data(csv_file, json_file):\n",
    "    try:\n",
    "        supplier_data = pd.read_csv(csv_file)\n",
    "        with open(json_file, 'r') as file:\n",
    "            brands_dict = json.load(file)\n",
    "\n",
    "        # Data filtering logic\n",
    "        filtered_data = [\n",
    "            {\n",
    "                \"SKU\": row['SKU'],\n",
    "                \"Name\": row['Name'],\n",
    "                \"Brand\": row['Brand'],\n",
    "                \"Cost\": row['Cost']\n",
    "            }\n",
    "            for _, row in supplier_data.iterrows() if row['Brand'] in brands_dict.values()\n",
    "        ]\n",
    "\n",
    "        return pd.DataFrame(filtered_data)\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing supplier data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to fetch BigCommerce products\n",
    "def fetch_bigcommerce_products(api):\n",
    "    products = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = api.get_request(f\"{api.base_url}catalog/products?page={page}&limit=250&include=variants\")\n",
    "        if response is None or 'data' not in response:\n",
    "            break\n",
    "\n",
    "        logging.info(f\"Fetched Page {page}: {len(response['data'])} products\")\n",
    "        products.extend(response['data'])\n",
    "\n",
    "        # Pagination logic\n",
    "        next_page = response.get('meta', {}).get('pagination', {}).get('next_page')\n",
    "        if not next_page:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    return products\n",
    "\n",
    "def fetch_and_process_products(api):\n",
    "    products = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = api.get_request(f\"{api.base_url}catalog/products?page={page}&limit=250&include=variants\")\n",
    "        if response is None or 'data' not in response:\n",
    "            break\n",
    "\n",
    "        logging.info(f\"Fetched Page {page}: {len(response['data'])} products\")\n",
    "        products.extend(response['data'])\n",
    "        next_page = response.get('meta', {}).get('pagination', {}).get('next_page')\n",
    "        if not next_page:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    all_product_frames = []\n",
    "    for product in products:\n",
    "        if 'variants' in product:\n",
    "            product_variants_df = pd.json_normalize(\n",
    "                product,\n",
    "                record_path='variants',\n",
    "                meta=[\n",
    "                    'id', \n",
    "                    'name', \n",
    "                    'price', \n",
    "                    'cost_price',\n",
    "                    ['custom_url', 'url']\n",
    "                ],\n",
    "                meta_prefix='product_',  # Prefix for product-level fields\n",
    "                record_prefix='variant_',  # Prefix for variant-level fields\n",
    "                errors='ignore'\n",
    "            )\n",
    "\n",
    "            # Rename columns to match your desired DataFrame structure\n",
    "            product_variants_df.rename(columns={\n",
    "                'product_id': 'ProductID',\n",
    "                'product_name': 'ProductName',\n",
    "                'variant_sku': 'SKU',\n",
    "                'product_custom_url.url': 'URL',\n",
    "                'product_price': 'ListPrice',\n",
    "                'product_cost_price': 'StoreCost'\n",
    "            }, inplace=True)\n",
    "\n",
    "            all_product_frames.append(product_variants_df)\n",
    "\n",
    "    if all_product_frames:\n",
    "        combined_df = pd.concat(all_product_frames, ignore_index=True)\n",
    "        return combined_df\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    try:\n",
    "        # API initialization\n",
    "        client_id = os.getenv(\"BIGCOMMERCE_CLIENT_ID\")\n",
    "        access_token = os.getenv(\"BIGCOMMERCE_ACCESS_TOKEN\")\n",
    "        store_hash = os.getenv(\"BIGCOMMERCE_STORE_HASH\")\n",
    "        api = BigCommerceAPI(client_id, access_token, store_hash)\n",
    "\n",
    "        # Fetching and processing products\n",
    "        products_data = fetch_bigcommerce_products(api)\n",
    "        if not products_data:\n",
    "            logging.error(\"No products data fetched from BigCommerce\")\n",
    "            return\n",
    "\n",
    "        # Processing fetched products data\n",
    "        products_df = fetch_and_process_products(api)\n",
    "        if products_df.empty:\n",
    "            logging.error(\"Processed products data is empty.\")\n",
    "            return\n",
    "\n",
    "        # Ensure that the 'ProductName' column exists before applying the filter\n",
    "        if 'ProductName' not in products_df.columns:\n",
    "            logging.error(\"'ProductName' column not found in products data\")\n",
    "            return\n",
    "\n",
    "        # Exclude products containing specific keywords\n",
    "        excluded_words = [\"Rebajado\", \"Oferta\", \"Promocion\", \"Antes\", \"Golpeado\"]\n",
    "        products_df = products_df[~products_df['ProductName'].apply(lambda x: contains_excluded_word(x, excluded_words))]\n",
    "        # Parse supplier data\n",
    "        supplier_csv = 'supplier.csv'\n",
    "        brands_json = 'brands.json'\n",
    "        suppliers_df = parse_supplier_data(supplier_csv, brands_json)\n",
    "\n",
    "        if suppliers_df.empty:\n",
    "            logging.error(\"Supplier data is empty or could not be processed.\")\n",
    "            return\n",
    "        # Apply fuzzy matching to find corresponding SKU in products_df for each supplier product\n",
    "        suppliers_df['MatchedName'] = suppliers_df['Name'].apply(match_product_name)\n",
    "        merged_df = pd.merge(suppliers_df, products_df, left_on='MatchedName', right_on='Name', how='left')\n",
    "\n",
    "        # Identifying new products, deleted products, and products with price updates\n",
    "        new_products = merged_df[merged_df['SKU'].isna()][['Name', 'SupplierPrice']]\n",
    "        new_products['Type'] = 'New Product'\n",
    "\n",
    "        # Fix for 'SettingWithCopyWarning'\n",
    "        deleted_products = products_df.loc[~products_df['ProductName'].isin(merged_df['ProductName']), :].copy()\n",
    "        deleted_products['Type'] = 'Deleted Product'\n",
    "\n",
    "        updated_products = merged_df.loc[~merged_df['SKU'].isna() & (merged_df['StoreCost'] != merged_df['SupplierPrice']), :].copy()\n",
    "        updated_products['Type'] = 'Product Updated'\n",
    "        # Preparing the final DataFrame\n",
    "        final_df = pd.concat([\n",
    "            new_products,\n",
    "            deleted_products[['SKU', 'Name', 'StoreCost', 'Type']],\n",
    "            updated_products[['SKU', 'Name', 'StoreCost', 'SupplierPrice', 'Type']]\n",
    "        ])\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "       # Save the final DataFrame to an Excel file\n",
    "        final_output_path = 'updated_product_list_final.xlsx'\n",
    "        final_df.to_excel(final_output_path, index=False)\n",
    "        logging.info(f\"Output saved to: {final_output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in main: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
